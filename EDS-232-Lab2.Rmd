---
title: "EDS-232-Lab2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab 2: Communities
## Lab 2a: Communities - Clusters

### Clustering

#### K-Means Clustering

##### Load and Plot `iris` dataset

```{r}
# load R packages
librarian::shelf(
  dplyr, DT, ggplot2, tibble)

# set seed for reproducible results
set.seed(42)

# load the dataset
data("iris")

# look at documentation in RStudio
if (interactive())
  help(iris)

# show data table
datatable(iris)
```

```{r}
# plot petal length vs width, species naive
ggplot(
  iris, aes(Petal.Length, Petal.Width)) +
  geom_point()
```

```{r}
# plot petal length vs width, color by species
legend_pos <- theme(
    legend.position = c(0.95, 0.05),
    legend.justification = c("right", "bottom"),
    legend.box.just = "right")
ggplot(
  iris, aes(Petal.Length, Petal.Width, color = Species)) +
  geom_point() +
  legend_pos
```

##### Cluster `iris` using `kmeans()`

```{r}
# cluster using kmeans
k <- 3  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# show cluster result
iris_k
```

```{r}
# compare clusters with species (which were not used to cluster)
table(iris_k$cluster, iris$Species)
```
**Question: How many observations could be considered “misclassified” if expecting petal length and width to differentiate between species?**

*There are a total of 6 observations that could potentially be "misclassified". 2 versicolor observations and 4 virginica observations.*

```{r}
# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos
```

**Question: Comparing the observed species plot with 3 species with the kmeans() cluster plot with 3 clusters, where does this “unsupervised” kmeans() technique (that does not use species to “fit” the model) produce similar versus different results? One or two sentences would suffice. Feel free to mention ranges of values along the axes.**

*The area on the clustered kmeans plot that doesn't match the "plotted by species" plot is right around where petal length (the x-axis) is equal to about 5. This is where two of the species of flowers have very similar traits, so it is difficult for the algorithm to differentiate between them.*

##### Plot Voronoi Diagram of clustered `iris`

```{r}
librarian::shelf(ggvoronoi, scales)

# define bounding box for geom_voronoi()
box <- tribble(
  ~Petal.Length, ~Petal.Width, ~group,
  1, 0.1, 1,
  1, 2.5, 1,
  7, 2.5, 1,
  7, 0.1, 1,
  1, 0.1, 1) %>% 
  data.frame()

# cluster using kmeans
k <- 3  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(iris_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```

**Task: Show the Voronoi diagram for fewer (k=2) and more (k=8) clusters to see how assignment to cluster centroids work.**

```{r}
# The Voronoi Diagram when k = 2

# cluster using kmeans
k <- 2  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(iris_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```
```{r}
# The Voronoi diagram when k = 8

# cluster using kmeans
k <- 8  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(iris_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```

#### Hierarchical Clustering

##### Load in the `dune` dataset

```{r}
librarian::shelf(
  cluster, vegan)

# load dune dataset from package vegan
data("dune")

# show documentation on dataset if interactive
if (interactive())
  help(dune)
```

**Question: What are the rows and columns composed of in the dune data frame?**

*In the `dune` dataframe the columns are the names of 30 different species, and the rows are the 20 sites that the species were observed at.*

##### Calculated ecological distances on `sites`

```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites
```

```{r}
sites_manhattan <- vegdist(sites, method="manhattan")
sites_manhattan
```

```{r}
sites_euclidean <- vegdist(sites, method="euclidean")
sites_euclidean
```

```{r}
sites_bray <- vegdist(sites, method="bray")
sites_bray
```

**Question: In your own words, how does Bray Curtis differ from Euclidean distance? See sites_euclidean versus sites_bray from lab code, slides from Lecture 05. Clustering and reading Chapter 8 of Kindt and Coe (2005).**

*Bray-Curtis and Euclidean distance are both measurements of how similar two sites are to one another. Euclidean distance is an actual measurement that doesn't have any boundries, and Bray-Curtis is just a scale of how similar two sites are from 0 to 1, with 1 being completely dissimilar and 0 being identical.*

##### Agglomerative hierarchical clustering on `dune`

```{r}
# Dissimilarity matrix
d <- vegdist(dune, method="bray")
dim(d)
```

```{r}
as.matrix(d)[1:5, 1:5]
```

```{r}
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Dendrogram plot of hc1
plot(hc1, cex = 0.6, hang = -1)
```

**Question: Which function comes first, vegdist() or hclust(), and why? See HOMLR 21.3.1 Agglomerative hierarchical clustering.**

*vegdist() comes before hclist(). This is because vegdist() gives us the actual distances between sites, then hclust() does the hierarchical clustering.*

```{r}
# Compute agglomerative clustering with agnes
hc2 <- agnes(dune, method = "complete")

# Agglomerative coefficient
hc2$ac
```

**Question: In your own words how does hclust() differ from agnes()? See HOMLR 21.3.1 Agglomerative hierarchical clustering and help documentation (?hclust(), ?agnes()).**

*hclust() and agnes() are very similar to each other, the only difference is that agnes() you also get the agglomerative coefficent, which measures the amount of clustering structure found.*

```{r}
# Dendrogram plot of hc2
plot(hc2, which.plot = 2)
```

```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dune, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)
```

**Question: Of the 4 methods, which is the “best” model in terms of Agglomerative Coefficient?**

*Here ward gives the strongest clustering structure.*

```{r}
# Compute ward linkage clustering with agnes
hc3 <- agnes(dune, method = "ward")

# Agglomerative coefficient
hc3$ac
```

```{r}
# Dendrogram plot of hc3
plot(hc3, which.plot = 2)
```

##### Divisive hierarchical clustering on `dune`

```{r}
# compute divisive hierarchical clustering
hc4 <- diana(dune)

# Divise coefficient; amount of clustering structure found
hc4$dc
```

**Question: In your own words how does agnes() differ from diana()? See HOMLR 21.3.1 Agglomerative hierarchical clustering, slides from Lecture 05. Clustering and help documentation (?agnes(), ?diana()).**

*agnes() and diana() are the opposite of eachother, but have similar functionality. Agnes starts from the bottom and goes up, meaning that it looks at each site as a leaf, then clusters it to other sites based on similarities. diana() is top down, meaning it starts at the root of all of the sites, and separates them based on disimilarities until each site is a leaf.*

##### Determining optimal clusters

```{r}
librarian::shelf(factoextra)

# Plot cluster results
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss",  k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

**Question: How do the optimal number of clusters compare between methods for those with a dashed line?**

*The two methods that have a dashed line show that 3 or 4 clusters is optimal, and the elbow method is similar in that the slope seems to get smaller right around 4 clusters. However it is a bit more difficult to see in the elbow method plot.*

##### Working with dendrograms

```{r}
# Construct dendorgram for the Ames housing example
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])
```

```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
k = 4
sub_grp <- cutree(hc5, k = k)

# Number of members in each cluster
table(sub_grp)
```

```{r}
# Plot full dendogram
fviz_dend(
  hc5,
  k = k,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco")
```

**Question: In dendrogram plots, which is the biggest determinant of relatedness between observations: the distance between observations along the labeled axes or the height of their shared connection? See HOMLR 21.5 Working with dendrograms.**

*In dendrograms, the biggest determinant of relatedness between observations is the height of their shared connection.*

## Lab 2b: Ordination

### Principal Components Analysis (PCA)

#### Prerequisites

```{r}
# load R packages
librarian::shelf(
  dplyr, ggplot2, h2o)
```

```{r}
# set seed for reproducible results
set.seed(42)

# get data
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)
```

```{r}
my_basket
```

#### Performing PCA in R

```{r}
h2o.no_progress()  # turn off progress bars for brevity
h2o.init(max_mem_size = "5g")  # connect to H2O instance
```

```{r}
# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)

# run PCA
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000)
my_pca
```

**Question: Why is the pca_method of “GramSVD” chosen over “GLRM”? See HOMLR 17.4 Performing PCA in R.**

*GramSVD is the better option to use when the data mostly contains numeric data. GLRM is better when there is a lot more categorical data.*

**Question: How many inital principal components are chosen with respect to dimensions of the input data? See HOMLR 17.4 Performing PCA in R.**

*There are the same amount of prinicpal components as there are features in the data, so in this case ncol(my_basket.h2o)*

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()
```

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text()
```

**Question: What category of grocery items contribute most to PC1? (These are related because they're bought most often together on a given grocery trip)**

*The category of grocery items that contribute most to PC1 is alcohol. It looks like it is closely followed by candy.*

**Question: What category of grocery items contribute the least to PC1 but positively towards PC2?**

*It looks like the category of grocery items that contribute least to PC1 but positivly towards PC2 is vegetables.*

#### Eigenvalue criterion

```{r}
# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>%
  as.vector() %>%
  .^2
  
# Sum of all eigenvalues equals number of variables
sum(eigen)
```

```{r}
## [1] 42

# Find PCs where the sum of eigenvalues is greater than or equal to 1
which(eigen >= 1)
```

```{r}
# Extract PVE and CVE
ve <- data.frame(
  PC  = my_pca@model$importance %>% seq_along(),
  PVE = my_pca@model$importance %>% .[2,] %>% unlist(),
  CVE = my_pca@model$importance %>% .[3,] %>% unlist())

# Plot PVE and CVE
ve %>%
  tidyr::gather(metric, variance_explained, -PC) %>%
  ggplot(aes(PC, variance_explained)) +
  geom_point() +
  facet_wrap(~ metric, ncol = 1, scales = "free")
```

**Question: How many principal components would you include to explain 90% of the total variance?**

*Looking at the CVE table we can see that in order to explain 90% of the total variance you would need to include about 36 principal components.*

```{r}
# How many PCs required to explain at least 75% of total variability
min(which(ve$CVE >= 0.75))
```

```{r}
# Screee plot criterion
data.frame(
  PC  = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>% .[2,] %>% unlist()) %>%
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)
```

**Question: How many principal components to include up to the elbow of the PVE, i.e. the “elbow” before plateau of dimensions explaining the least variance?**

*8 principal components.*

**Question: What are a couple of disadvantages to using PCA? See HOMLR 17.6 Final thoughts.**

*A few disadvantages of PCA are first, PCA can be highly affected by outliers. Second, PCA does not perform as well in very high dimensional space where complex nonlinear patterns often exist.*

### Non-metric MultiDimensional Scaling (NMDS)

#### Unconstrained Ordination on Species

```{r}
# load R packages
librarian::shelf(
  vegan, vegan3d)
```

```{r}
# vegetation and environment in lichen pastures from Vare et al (1995)
data("varespec") # species
data("varechem") # chemistry

varespec %>% tibble()
```

**Question: What are the dimensions of the varespec data frame and what do rows versus columns represent?**

*The dimensions of the varspec data frame are 24 x 44, and the columns represent each species that was observed, and the rows are each site that they were observed at.*

```{r}
vare.dis <- vegdist(varespec)
vare.mds0 <- monoMDS(vare.dis)
stressplot(vare.mds0)
```

**Question: The “stress” in a stressplot represents the difference between the observed inpnut distance versus the fitted ordination distance. How much better is the non-metric (i.e., NMDS) fit versus a linear fit (as with PCA) in terms of \(R^2\)?**

*The R^2 value with the non-metric fit is 0.99, and the R^2 value with the linear fit is 0.943.*

```{r}
ordiplot(vare.mds0, type = "t")
```

**Question: What two sites are most dissimilar based on species composition for the first component MDS1? And two more most dissimilar sites for the second component MDS2?**

*The two sites that are most dissimilar based on MDS1 are sites 28 and 5. The two sites that are most dissimilar based on MDS2 are sites 21 and 14.*

```{r}
vare.mds <- metaMDS(varespec, trace = FALSE)
vare.mds
```

**Question: What is the basic difference between metaMDS and monoMDS()? See 2.1 Non-metric Multidimensional scaling of vegantutor.pdf.**

*The basic difference between metamMDS and monoMDS is that monoMDS only has one random start and metaMDS has many random starts.*

```{r}
plot(vare.mds, type = "t")
```

#### Overlay with Environment

```{r}
ef <- envfit(vare.mds, varechem, permu = 999)
ef
```

```{r}
plot(vare.mds, display = "sites")
plot(ef, p.max = 0.05)
```

**Question: What two soil chemistry elements have the strongest negative relationship with NMDS1 that is based on species composition?**

*Al and Fe have the strongest negative relationship with NMDS1.*

```{r}
ef <- envfit(vare.mds ~ Al + Ca, data = varechem)
plot(vare.mds, display = "sites")
plot(ef)

tmp <- with(varechem, ordisurf(vare.mds, Al, add = TRUE))
ordisurf(vare.mds ~ Ca, data=varechem, add = TRUE, col = "green4")
```

**Question: Which of the two NMDS axes differentiates Ca the most, i.e. has the highest value given by the contours at the end (and not middle) of the axis?**

*NMDS1*

#### Constrained Ordination on Species and Environment

```{r}
# ordinate on species constrained by three soil elements
vare.cca <- cca(varespec ~ Al + P + K, varechem)
vare.cca
```

**Question: What is the difference between “constrained” versus “unconstrained” ordination within ecological context?**

*With unconstrained ordination we first find the major compositional variation, and then relate this variation to observed environmental variation. With constrained variation we do not want to display all or even most of the compositional variation, but only the variation that can be explained by the used environmental variables, or constraints.*

```{r}
# plot ordination
plot(vare.cca)
```

**Question: What sites are most differentiated by CCA1, i.e. furthest apart along its axis, based on species composition AND the environmnent? What is the strongest environmental vector for CCA1, i.e. longest environmental vector in the direction of the CCA1 axes?**

*Sites 28 and 4 are most differentiated by CCA1 based on species composition and the environment. The strongest environmental vector for CCA1 is Al.*

```{r}
# plot 3 dimensions
ordiplot3d(vare.cca, type = "h")
```

```{r}
if (interactive()){
  ordirgl(vare.cca)
}
```

